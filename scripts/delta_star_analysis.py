#MEMO
# Ï¥àÍ∏∞ ÏãúÏûëÏ†ê ÌôïÏù∏ 

################################################################################################################################################
################################################################################################################################################
################################################################################################################################################


# Œ¥* (delta-star) ÏûÑÍ≥ÑÍ∞í ÌÉêÏÉâ Ïã§Ìóò: ÏãúÍ∞ÑÏ†Å Ìï†Ïù∏ ÏöîÏù∏ÏùÑ ÌÜµÌïú ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÑ±Îä• ÎπÑÍµê
"""
This script implements an experiment to find the critical temporal discount factor Œ¥*
where clustered and random networks perform equally in complex contagion simulations.

Key Concepts:
- Œ¥ (delta): Temporal discount factor ranging from 0 to 1
  - When Œ¥ is close to 0: Only immediate spread matters
  - When Œ¥ is close to 1: All time steps are valued equally

- V: Discounted cumulative activation size
  - Formula: V = c‚ÇÅ√óŒ¥ + c‚ÇÇ√óŒ¥¬≤ + c‚ÇÉ√óŒ¥¬≥ + ... + c‚Çú√óŒ¥·µó
  - where c‚Çú is the number of newly activated nodes at time step t

- Œ¥*: Critical threshold where V_clustered = V_random
  - Below Œ¥*: Clustered networks outperform (reinforcement matters more)
  - Above Œ¥*: Random networks outperform (reach matters more)
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from helper_functions import (
    sirsic_wmem, run_simulation,
    seed_strat_one_nbrs, seed_strat_random, seed_strat_adj
)

###################
# Core Functions for Œ¥* Analysis

def cumulative_to_new_infections(cumulative_series):
    """
    Convert cumulative adoption counts to per-timestep new infections.

    Parameters:
        cumulative_series: list of integers
            Cumulative number of adopters at each time step
            Example: [5, 8, 12, 15, 15, 15] means 5 at t=0, 8 at t=1, etc.

    Returns:
        new_infections: list of integers
            Number of newly infected nodes at each time step
            Example: [5, 3, 4, 3, 0, 0] for the above input
    """
    # Initialize with the first value (number of initial seeds)
    # This represents how many nodes adopted at timestep 0
    new_infections = [cumulative_series[0]]

    # Loop through remaining timesteps to calculate new adoptions
    for t in range(1, len(cumulative_series)):
        # Calculate new infections = cumulative at t - cumulative at t-1
        # This gives us how many NEW nodes adopted at this specific timestep
        new_at_t = cumulative_series[t] - cumulative_series[t-1]
        new_infections.append(new_at_t)

    return new_infections


def calculate_V(new_infections, delta):
    """
    Calculate the discounted cumulative activation size V.

    This metric applies temporal discounting to value early adoptions more heavily
    than later ones (when delta < 1).

    Formula: V = Œ£(c‚Çú √ó Œ¥·µó) for t = 1, 2, 3, ...

    Parameters:
        new_infections: list of integers
            Number of newly activated nodes at each time step (c‚Çú values)
        delta: float between 0 and 1
            Temporal discount factor
            - delta = 0: Only first timestep matters
            - delta = 1: All timesteps weighted equally (equivalent to total spread)

    Returns:
        V: float
            Discounted cumulative activation size

    Example:
        If new_infections = [5, 3, 4, 2] and delta = 0.9:
        V = 5√ó0.9¬π + 3√ó0.9¬≤ + 4√ó0.9¬≥ + 2√ó0.9‚Å¥
          = 5√ó0.9 + 3√ó0.81 + 4√ó0.729 + 2√ó0.6561
          = 4.5 + 2.43 + 2.916 + 1.3122
          = 11.1582
    """
    # Initialize the discounted value V to zero
    V = 0.0

    # Loop through each timestep and apply exponential discounting
    for t in range(len(new_infections)):
        # Apply discount factor: delta^(t+1)
        # - t+1 is used because we start counting from t=0 but discount from time 1
        # - Lower delta values make later adoptions worth less
        # - When delta=0.9, adoptions at t=0 are worth 0.9, at t=1 worth 0.81, etc.
        V += new_infections[t] * (delta ** (t + 1))

    return V


def run_delta_simulations(k=4, n=None, thrshld=2, p1=0.1, p2=0.7,
                          beta=5, trials=20, seeds=2):
    """
    Run simulations for both clustered and random networks with full time series.

    Parameters:
        k: int
            Network degree (default: 8)
        n: int or None
            Number of nodes. If None, uses k*250 (default: None)
        thrshld: int
            Social reinforcement threshold (default: 2)
        p1: float
            Base adoption probability (default: 0.3)
        p2: float
            Reinforced adoption probability (default: 0.6)
        beta: int
            Time of influence (default: 1)
        trials: int
            Number of simulation trials per network type (default: 20)
        seeds: int
            Number of initial seeds (default: 5)

    Returns:
        results_clustered: DataFrame
            Simulation results for clustered network (perc_rewire=0)
        results_random: DataFrame
            Simulation results for random network (perc_rewire=1)
    """
    # Set default network size if not specified (k*250 is a common convention)
    if n is None:
        n = k * 250

    # Display simulation parameters for user verification
    print(f"Setting up simulations with parameters:")
    print(f"  Network: k={k}, n={n}")
    print(f"  Diffusion: i={thrshld}, p1={p1}, p2={p2}, beta={beta}")
    print(f"  Trials: {trials} per network type\n")

    # Create base network using Watts-Strogatz model
    # - n: number of nodes
    # - k: each node connects to k nearest neighbors
    # - 0: rewiring probability (0 = no rewiring, creates regular ring lattice)
    # - seed=42: fixed seed for reproducibility
    print("Creating base network (WS ring lattice)...")
    G = nx.connected_watts_strogatz_graph(n, k, 0, seed=42)

    # Build the adoption probability trajectory
    # - prob_list[i] = probability of adoption when exposed to i influential neighbors
    # - [0]: Can't adopt with 0 exposures
    # - [p1] * (thrshld-1): Low probability (p1) below threshold
    # - [p2] * n: High probability (p2) at or above threshold
    # Example with thrshld=2: [0, p1, p2, p2, p2, ...]
    prob_list = [0] + [p1] * (thrshld - 1) + [p2] * n

    # Run simulations for CLUSTERED network (perc_rewire = 0)
    # Clustered networks have high local clustering (friends-of-friends know each other)
    print("Running simulations on CLUSTERED network (perc_rewire=0)...")
    _, cf_clustered = run_simulation(
        G=G,                              # Base network structure
        G_name='WS',                      # Watts-Strogatz network type
        model=sirsic_wmem,                # Diffusion model with memory
        trials=trials,                    # Number of independent simulation runs
        r_start=0,                        # Starting trial number
        beta=beta,                        # Time of influence (memory parameter)
        prob_list=prob_list,              # Adoption probabilities by exposure count
        perc=0,                           # 0 = no rewiring = CLUSTERED network
        rand_path=None,                   # No pre-saved random network file
        seeds=seeds,                      # Number of initial adopters
        seed_strat0=seed_strat_one_nbrs,  # First seeding: select neighbor pairs
        seed_strat=seed_strat_random,     # Subsequent seeding: random selection
        sig_thresh_sd=0,                  # 0 = no heterogeneity in thresholds
        full_series=True,                 # CRITICAL: Save full time series for V calculation
        rand_network='ES'                 # 'ES' = Edge Swap randomization method
    )

    # Run simulations for RANDOM network (perc_rewire = 1)
    # Random networks have low clustering but shorter path lengths (more reach)
    print("Running simulations on RANDOM network (perc_rewire=1)...")
    _, cf_random = run_simulation(
        G=G,                              # Same base network structure
        G_name='WS',
        model=sirsic_wmem,
        trials=trials,
        r_start=0,
        beta=beta,
        prob_list=prob_list,
        perc=1,                           # 1 = full rewiring = RANDOM network
        rand_path=None,
        seeds=seeds,
        seed_strat0=seed_strat_one_nbrs,
        seed_strat=seed_strat_random,
        sig_thresh_sd=0,
        full_series=True,                 # CRITICAL: Save full time series for V calculation
        rand_network='ES'
    )

    print(f"\nSimulations complete!")
    print(f"  Clustered trials: {len(cf_clustered)}")
    print(f"  Random trials: {len(cf_random)}")

    return cf_clustered, cf_random


def compute_V_across_deltas(results_df, delta_values):
    """
    Compute V (discounted cumulative activation) for each trial across different Œ¥ values.

    Parameters:
        results_df: DataFrame
            Simulation results containing 'full_timeseries' column
        delta_values: list of floats
            Delta values to test (e.g., [0.5, 0.7, 0.9, 0.95, 0.99])

    Returns:
        V_matrix: numpy array of shape (n_trials, n_deltas)
            V values for each trial (rows) and delta value (columns)
    """
    # Get dimensions for the output matrix
    n_trials = len(results_df)
    n_deltas = len(delta_values)

    # Initialize matrix to store V values
    # Rows = individual trials, Columns = different delta values
    V_matrix = np.zeros((n_trials, n_deltas))

    # Loop through each simulation trial
    for trial_idx in range(n_trials):
        # Extract the cumulative adoption time series for this trial
        # This is a list like [5, 8, 12, 15, 15, ...] showing total adopters over time
        cumulative_series = results_df.iloc[trial_idx]['full_timeseries']

        # Convert cumulative counts to new infections per timestep
        # Transforms [5, 8, 12, 15] ‚Üí [5, 3, 4, 3]
        new_infections = cumulative_to_new_infections(cumulative_series)

        # Calculate V for each delta value using the same trial data
        # This shows how the same diffusion trajectory is valued under different time preferences
        for delta_idx, delta in enumerate(delta_values):
            V = calculate_V(new_infections, delta)
            V_matrix[trial_idx, delta_idx] = V

    return V_matrix


def find_delta_star(delta_values, V_clustered_mean, V_random_mean, n, min_delta=0.0001):
    """
    Find ALL Œ¥* values where clustered and random networks have equal performance.

    Uses linear interpolation to find intersection points, filtering out:
    1. Trivial intersections near delta=0
    2. Intersections in the "full spread" regime (V > 60% of population)

    Parameters:
        delta_values: array-like
            Delta values tested
        V_clustered_mean: array-like
            Mean V values for clustered network at each delta
        V_random_mean: array-like
            Mean V values for random network at each delta
        n: int
            Network size (number of nodes) for calculating 60% threshold
        min_delta: float
            Minimum delta threshold to exclude trivial intersections (default: 0.0001)

    Returns:
        tuple: (primary_delta_star, all_intersections)
            - primary_delta_star: float or None
                The first meaningful delta* value (None if no intersection found)
            - all_intersections: list of dicts
                List of all meaningful intersections, each containing:
                    'delta_star': float - the intersection value
                    'interval': tuple - (lower_delta, upper_delta) bracketing the intersection
                    'transition': str - direction of transition ('clustered_to_random' or 'random_to_clustered')
                    'V_at_intersection': float - interpolated V value at delta*
                    'is_valid': bool - whether V <= 60% threshold (minimal spread regime)
    """
    # Calculate the 60% threshold for "minimal spread" regime
    # Only intersections below this threshold are considered valid
    spread_threshold = n * 0.6

    # Convert to numpy arrays for easier interpolation
    delta_values = np.array(delta_values)
    V_clustered_mean = np.array(V_clustered_mean)
    V_random_mean = np.array(V_random_mean)

    # Calculate the difference between random and clustered networks at each delta
    # Positive values = random network performing better
    # Negative values = clustered network performing better
    diff = V_random_mean - V_clustered_mean

    # Find where the difference changes sign (crosses zero)
    # np.diff(np.sign(diff)) will be non-zero where sign changes
    # This identifies the interval where the curves intersect
    sign_changes = np.where(np.diff(np.sign(diff)))[0]

    # List to store all intersections (both valid and invalid for tracking)
    all_intersections = []
    valid_intersections = []

    # Process each sign change to find exact intersection points
    for idx in sign_changes:
        # Linear interpolation to find the exact delta* value
        # We know the intersection is between delta_values[idx] and delta_values[idx+1]
        x1, x2 = delta_values[idx], delta_values[idx + 1]  # Delta values bracketing the intersection
        y1, y2 = diff[idx], diff[idx + 1]                   # Difference values at those deltas

        # Use linear interpolation formula to find where the line crosses y=0
        # Formula: x* = x1 - y1 * (x2 - x1) / (y2 - y1)
        # This solves: y = y1 + (y2-y1)/(x2-x1) * (x - x1) = 0 for x
        delta_star_candidate = x1 - y1 * (x2 - x1) / (y2 - y1)

        # Interpolate V value at delta* to check 60% threshold
        # Since this is the intersection point, V_clustered ‚âà V_random at delta*
        # We use V_clustered for the check (could use either since they're equal)
        V_c1, V_c2 = V_clustered_mean[idx], V_clustered_mean[idx + 1]
        V_at_intersection = V_c1 + (V_c2 - V_c1) * (delta_star_candidate - x1) / (x2 - x1)

        # Check validity criteria
        is_valid = (delta_star_candidate >= min_delta) and (V_at_intersection <= spread_threshold)

        # Determine transition direction
        # If diff goes from negative to positive: clustered was better, now random is better
        # If diff goes from positive to negative: random was better, now clustered is better
        if y1 < 0 and y2 > 0:
            transition = 'clustered_to_random'
        elif y1 > 0 and y2 < 0:
            transition = 'random_to_clustered'
        else:
            # Handle edge case where one of the values is exactly zero
            transition = 'clustered_to_random' if y2 > y1 else 'random_to_clustered'

        # Store this intersection with validity info
        intersection_dict = {
            'delta_star': delta_star_candidate,
            'interval': (x1, x2),
            'transition': transition,
            'V_at_intersection': V_at_intersection,
            'is_valid': is_valid
        }

        all_intersections.append(intersection_dict)

        # Only keep valid intersections for return
        if is_valid:
            valid_intersections.append(intersection_dict)

    # Print detailed analysis of intersections
    print("\n" + "="*70)
    print("INTERSECTION ANALYSIS")
    print("="*70)

    # Report all intersections found (valid + invalid)
    if len(all_intersections) == 0:
        print(f"Found 0 intersections in the given delta range.")
        print("="*70 + "\n")
        return None, []

    print(f"Found {len(all_intersections)} total intersection(s):\n")

    # Print details for each intersection, marking valid vs invalid
    for i, intersection in enumerate(all_intersections, 1):
        validity_str = "‚úì VALID" if intersection['is_valid'] else "‚úó INVALID"
        print(f"Intersection {i}: {validity_str}")
        print(f"  Œ¥* = {intersection['delta_star']:.4f}")
        print(f"  V at Œ¥* = {intersection['V_at_intersection']:.2f} (threshold: {spread_threshold:.2f})")
        print(f"  Interval: [{intersection['interval'][0]:.4f}, {intersection['interval'][1]:.4f}]")

        # Format transition direction for readability
        if intersection['transition'] == 'clustered_to_random':
            print(f"  Transition: Clustered ‚Üí Random (clustered advantage ends)")
        else:
            print(f"  Transition: Random ‚Üí Clustered (random advantage ends)")

        # Explain why invalid if applicable
        if not intersection['is_valid']:
            if intersection['delta_star'] < min_delta:
                print(f"  Reason: Œ¥* < {min_delta} (trivial intersection)")
            elif intersection['V_at_intersection'] > spread_threshold:
                print(f"  Reason: V > {spread_threshold:.2f} (full spread regime)")
        print()

    # Report valid intersections
    print(f"Valid intersections (minimal spread regime): {len(valid_intersections)}")

    if len(valid_intersections) == 0:
        print("\n‚ö† No valid delta* found!")
        print(f"  All intersections were filtered out:")
        print(f"    - Trivial (Œ¥ < {min_delta}): {sum(1 for i in all_intersections if i['delta_star'] < min_delta)}")
        print(f"    - Full spread (V > {spread_threshold:.2f}): {sum(1 for i in all_intersections if i['V_at_intersection'] > spread_threshold)}")
        print("="*70 + "\n")
        return None, []

    # Check for multiple valid intersections and warn about non-monotonic behavior
    if len(valid_intersections) > 1:
        print("\n‚ö† WARNING: MULTIPLE valid intersections detected - complex behavior!")
        print("  The relative performance of networks is NON-MONOTONIC with Œ¥.")
        print("  This indicates that the optimal network structure changes multiple")
        print("  times as the temporal preference shifts.")
        print(f"  Recommending PRIMARY Œ¥* = {valid_intersections[0]['delta_star']:.4f} (first valid transition)")

    print("="*70 + "\n")

    # Return the first valid intersection as primary (for backward compatibility)
    # along with the complete list of valid intersections
    primary_delta_star = valid_intersections[0]['delta_star']
    return primary_delta_star, valid_intersections


def determine_dominant_network(V_clustered_mean, V_random_mean, valid_intersections):
    """
    Determine which network type is dominant and when.

    Parameters:
        V_clustered_mean: array-like
            Mean V values for clustered network at each delta
        V_random_mean: array-like
            Mean V values for random network at each delta
        valid_intersections: list of dicts
            Valid delta* intersections (after filtering)

    Returns:
        dict with keys:
            - 'delta_star_count': int (0, 1, or 2+)
            - 'delta_star_values': str (formatted string of delta* values)
            - 'dominant_below': str or None (for count=1 only)
            - 'dominant_above': str or None (for count=1 only)
            - 'always_dominant': str or None (for count=0 only)
    """
    count = len(valid_intersections)

    # Initialize result dict
    result = {
        'delta_star_count': count,
        'delta_star_values': None,
        'dominant_below': None,
        'dominant_above': None,
        'always_dominant': None
    }

    if count == 0:
        # No delta* found - one network always dominates
        # Calculate average difference across all delta values
        diff = np.array(V_random_mean) - np.array(V_clustered_mean)
        avg_diff = np.mean(diff)

        if avg_diff > 0:
            result['always_dominant'] = 'Random'
        else:
            result['always_dominant'] = 'Clustered'

        result['delta_star_values'] = 'None'

    elif count == 1:
        # Exactly one delta* - clear transition
        delta_star = valid_intersections[0]['delta_star']
        transition = valid_intersections[0]['transition']

        result['delta_star_values'] = f"{delta_star:.4f}"

        # Determine which network dominates before and after delta*
        if transition == 'clustered_to_random':
            result['dominant_below'] = 'Clustered'
            result['dominant_above'] = 'Random'
        else:  # 'random_to_clustered'
            result['dominant_below'] = 'Random'
            result['dominant_above'] = 'Clustered'

    else:
        # Multiple delta* values - complex behavior
        # Just list the values, don't determine dominance
        delta_star_list = [f"{intersection['delta_star']:.4f}" for intersection in valid_intersections]
        result['delta_star_values'] = '[' + ', '.join(delta_star_list) + ']'

        # Leave dominant_below, dominant_above, always_dominant as None
        # User needs to interpret the complex pattern themselves

    return result


def visualize_delta_analysis(delta_values, V_clustered_mean, V_clustered_se,
                             V_random_mean, V_random_se, delta_star=None, all_intersections=None):
    """
    Create visualization showing how V changes with Œ¥ for both network types.

    Parameters:
        delta_values: array-like
            Delta values tested
        V_clustered_mean: array-like
            Mean V for clustered networks
        V_clustered_se: array-like
            Standard error for clustered networks
        V_random_mean: array-like
            Mean V for random networks
        V_random_se: array-like
            Standard error for random networks
        delta_star: float or None
            Primary critical delta value to mark on plot (for backward compatibility)
        all_intersections: list of dicts or None
            List of all intersection dictionaries with keys 'delta_star', 'interval', 'transition'
    """
    # Create figure and axis objects
    fig, ax = plt.subplots(figsize=(10, 6))

    # Plot clustered network performance curve
    # '-' = line only (no markers)
    ax.plot(delta_values, V_clustered_mean, '-', color='blue',
            linewidth=2, label='Clustered Network')
    # Add shaded region for standard error (uncertainty band)
    ax.fill_between(delta_values,
                    V_clustered_mean - V_clustered_se,  # Lower bound
                    V_clustered_mean + V_clustered_se,  # Upper bound
                    color='blue', alpha=0.2)            # Semi-transparent blue

    # Plot random network performance curve
    # '-' = line only (no markers)
    ax.plot(delta_values, V_random_mean, '-', color='red',
            linewidth=2, label='Random Network')
    # Add shaded region for standard error
    ax.fill_between(delta_values,
                    V_random_mean - V_random_se,
                    V_random_mean + V_random_se,
                    color='red', alpha=0.2)

    # Mark the critical threshold(s) Œ¥* if found
    if delta_star is not None and all_intersections is not None:
        # Draw all intersections with appropriate styling
        for i, intersection in enumerate(all_intersections):
            delta_val = intersection['delta_star']

            if i == 0:
                # PRIMARY intersection: thick black dashed line with large label
                ax.axvline(delta_val, color='black', linestyle='--', linewidth=2.5,
                          label=f'Œ¥* = {delta_val:.3f}', zorder=10)
                # Add prominent text label at the top
                ax.text(delta_val, ax.get_ylim()[1] * 0.95, f'  Œ¥* = {delta_val:.3f}',
                       rotation=90, verticalalignment='top', fontsize=12,
                       fontweight='bold', color='black', zorder=11)
            else:
                # SECONDARY intersections: thinner gray dotted lines with smaller labels
                ax.axvline(delta_val, color='gray', linestyle=':', linewidth=1.5,
                          alpha=0.7, label=f'Œ¥*‚Çç{i+1}‚Çé = {delta_val:.3f}', zorder=9)
                # Add smaller text label
                ax.text(delta_val, ax.get_ylim()[1] * 0.88, f'  Œ¥*‚Çç{i+1}‚Çé = {delta_val:.3f}',
                       rotation=90, verticalalignment='top', fontsize=10,
                       color='gray', alpha=0.8, zorder=9)

    elif delta_star is not None:
        # Backward compatibility: single delta_star provided without all_intersections
        # Draw vertical dashed line at Œ¥*
        ax.axvline(delta_star, color='black', linestyle='--', linewidth=2,
                  label=f'Œ¥* = {delta_star:.3f}')
        # Add text label at the top of the line
        ax.text(delta_star, ax.get_ylim()[1] * 0.95, f'  Œ¥* = {delta_star:.3f}',
               rotation=90, verticalalignment='top', fontsize=12, fontweight='bold')
    else:
        # Œ¥* was not found - add a prominent warning annotation
        # Position: center-top of the plot for maximum visibility
        # Get the middle x position (in data coordinates)
        x_mid = (delta_values[0] + delta_values[-1]) / 2
        # Get the upper y position (95% of the way up)
        y_top = ax.get_ylim()[1] * 0.95

        # Create the warning text with line breaks for readability
        warning_text = "‚ö† Œ¥* NOT FOUND ‚ö†\nNo intersection in tested range\n(Consider expanding delta values)"

        # Add the text annotation with a highly visible yellow box and red text
        ax.text(x_mid, y_top, warning_text,
               horizontalalignment='center',      # Center the text horizontally
               verticalalignment='top',           # Anchor at top
               fontsize=13,                       # Large, readable font
               fontweight='bold',                 # Bold for emphasis
               color='darkred',                   # Dark red text for warning
               bbox=dict(
                   boxstyle='round,pad=0.8',      # Rounded corners with padding
                   facecolor='yellow',            # Yellow background (warning color)
                   edgecolor='red',               # Red border for extra emphasis
                   linewidth=3,                   # Thick border
                   alpha=0.9                      # Slightly transparent
               ),
               zorder=100)                        # Draw on top of everything

    # Set axis labels with formatting
    ax.set_xlabel('Temporal Discount Factor (Œ¥)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Discounted Cumulative Activation (V)', fontsize=14, fontweight='bold')
    ax.set_title('Network Performance vs. Temporal Discounting\n' +
                'Finding Œ¥*: The Critical Threshold', fontsize=16, fontweight='bold')

    # Add legend to identify the curves
    ax.legend(fontsize=12, loc='best')

    # Add grid for easier reading
    ax.grid(True, alpha=0.3)

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Create output directory if it doesn't exist
    os.makedirs('../output_plots', exist_ok=True)

    # Save the figure as high-resolution PNG
    plt.savefig('../output_plots/delta_star_analysis.png', dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved to: output_plots/delta_star_analysis.png")

    # Display the plot
    plt.show()

    return fig, ax


###################
# Main Execution

def main(k=4, n=None, thrshld=2, p1=0.1, p2=1.0, beta=5, trials=50, seeds=None,
         delta_values=None, verbose=True):
    """
    Main function to run the complete Œ¥* analysis.

    Parameters:
        k: int
            Network degree (default: 4)
        n: int or None
            Number of nodes. If None, uses k*250 (default: None)
        thrshld: int
            Social reinforcement threshold (default: 4)
        p1: float
            Base adoption probability (default: 0.75)
        p2: float
            Reinforced adoption probability (default: 0.75)
        beta: int
            Time of influence (default: 1)
        trials: int
            Number of simulation trials per network type (default: 50)
        seeds: int or None
            Number of initial seeds. If None, uses thrshld (default: None)
        delta_values: list or None
            Delta values to test. If None, uses np.linspace(0, 1, 51) (default: None)
        verbose: bool
            If True, print detailed output. If False, suppress most output (default: True)

    Returns:
        tuple: (delta_star, results_summary, all_intersections)
            - delta_star: float or None (primary intersection)
            - results_summary: DataFrame with detailed results
            - all_intersections: list of all intersection dicts

    This function orchestrates the entire workflow:
    1. Define simulation parameters
    2. Run simulations on both network types
    3. Calculate V for different Œ¥ values
    4. Find the critical threshold Œ¥*
    5. Visualize and save results
    """
    # Set defaults for None parameters
    if n is None:
        n = k * 250
    if seeds is None:
        seeds = thrshld
    if delta_values is None:
        delta_values = np.linspace(0, 1, 1001).tolist()

    # Suppress output if not verbose
    import sys
    import io
    old_stdout = None
    if not verbose:
        # Redirect stdout to suppress print statements
        old_stdout = sys.stdout
        sys.stdout = io.StringIO()

    try:
        # Print header for the analysis
        print("="*70)
        print("Œ¥* (DELTA-STAR) ANALYSIS")
        print("Finding the critical temporal discount threshold")
        print("="*70 + "\n")

        # ========== STEP 1: Define parameters ==========
        print("STEP 1: Setting parameters")
        print("-" * 50)
        print(f"Delta values to test: {delta_values}\n")

        # ========== STEP 2: Run simulations ==========
        print("STEP 2: Running simulations")
        print("-" * 50)

        # Run simulations on both clustered (perc=0) and random (perc=1) networks
        # This will give us time series data for each trial
        cf_clustered, cf_random = run_delta_simulations(
            k=k, n=n, thrshld=thrshld, p1=p1, p2=p2,
            beta=beta, trials=trials, seeds=seeds
        )

        # ========== STEP 3: Calculate V for different delta values ==========
        print("\nSTEP 3: Calculating V for different Œ¥ values")
        print("-" * 50)

        # For clustered network: compute V at each delta value for all trials
        # This creates a matrix: rows=trials, columns=delta values
        print("Computing V for clustered network...")
        V_clustered = compute_V_across_deltas(cf_clustered, delta_values)

        # Same for random network
        print("Computing V for random network...")
        V_random = compute_V_across_deltas(cf_random, delta_values)

        # Calculate summary statistics across trials
        # Mean: average performance at each delta
        V_clustered_mean = np.mean(V_clustered, axis=0)
        # Standard error: uncertainty in the mean estimate
        V_clustered_se = np.std(V_clustered, axis=0) / np.sqrt(trials)

        V_random_mean = np.mean(V_random, axis=0)
        V_random_se = np.std(V_random, axis=0) / np.sqrt(trials)

        # Print results table showing performance comparison at each delta
        print("\nResults Summary:")
        print("=" * 70)
        print(f"{'Œ¥':>8} | {'V_clustered':>15} | {'V_random':>15} | {'Difference':>15}")
        print("-" * 70)
        for i, delta in enumerate(delta_values):
            # Calculate difference: positive means random is better, negative means clustered is better
            diff = V_random_mean[i] - V_clustered_mean[i]
            winner = "Random" if diff > 0 else "Clustered"
            print(f"{delta:>8.2f} | {V_clustered_mean[i]:>15.2f} | "
            f"{V_random_mean[i]:>15.2f} | {diff:>+15.2f} ({winner})")
        print("=" * 70)

        # ========== STEP 4: Find delta* ==========
        print("\nSTEP 4: Finding Œ¥* (critical threshold)")
        print("-" * 50)

        # Find ALL delta values where the two curves intersect
        # This is the critical threshold (or thresholds) that separate the two regimes
        # Pass n for 60% threshold filtering
        delta_star, all_intersections = find_delta_star(delta_values, V_clustered_mean, V_random_mean, n)

        # Interpret the result
        if delta_star is not None:
            print(f"\n*** PRIMARY Œ¥* found: {delta_star:.4f} ***")

            # Check for suspiciously low delta_star values
            if delta_star < 0.1:
                print(f"\n‚ö† WARNING: Œ¥* = {delta_star:.4f} is suspiciously LOW!")
                print(f"  This indicates an extreme early-adoption regime.")
                print(f"  Potential issues to check:")
                print(f"    - Is the minimum threshold (0.05) too permissive?")
                print(f"    - Are the simulation parameters creating trivial dynamics?")
                print(f"    - Consider validating with different parameter settings.")
                print()

            # Check for multiple intersections and provide extended interpretation
            if len(all_intersections) > 1:
                print(f"\n‚ö† COMPLEX BEHAVIOR DETECTED:")
                print(f"  Multiple intersections indicate NON-MONOTONIC relative performance.")
                print(f"  The optimal network structure CHANGES {len(all_intersections)} times as Œ¥ varies.")
                print(f"\nDetailed regime breakdown:")

                # Add beginning regime
                first_transition = all_intersections[0]['transition']
                if first_transition == 'clustered_to_random':
                    print(f"  1. Œ¥ < {all_intersections[0]['delta_star']:.4f}: Clustered networks dominate")
                    print(f"     (High clustering advantage in early-adoption regime)")
                else:
                    print(f"  1. Œ¥ < {all_intersections[0]['delta_star']:.4f}: Random networks dominate")
                    print(f"     (High reach advantage in early-adoption regime)")

                # Add intermediate regimes
                for i in range(len(all_intersections)):
                    intersection = all_intersections[i]
                    if i < len(all_intersections) - 1:
                        next_intersection = all_intersections[i + 1]
                        # Determine which network wins in this interval
                        if intersection['transition'] == 'clustered_to_random':
                            winner = "Random"
                            reason = "Reach advantage dominates"
                        else:
                            winner = "Clustered"
                            reason = "Reinforcement advantage dominates"

                        print(f"  {i+2}. {intersection['delta_star']:.4f} < Œ¥ < {next_intersection['delta_star']:.4f}: {winner} networks dominate")
                        print(f"     ({reason})")
                    else:
                        # Final regime (after last intersection)
                        if intersection['transition'] == 'clustered_to_random':
                            print(f"  {i+2}. Œ¥ > {intersection['delta_star']:.4f}: Random networks dominate")
                            print(f"     (Reach advantage in late-adoption regime)")
                        else:
                            print(f"  {i+2}. Œ¥ > {intersection['delta_star']:.4f}: Clustered networks dominate")
                            print(f"     (Reinforcement advantage in late-adoption regime)")

                print(f"\n  üí° Interpretation: This non-monotonic pattern suggests complex")
                print(f"     interaction between timing, reinforcement, and reach effects.")
                print(f"     The 'best' network depends critically on temporal preferences.")

            else:
                # Single intersection - standard interpretation
                print(f"\nInterpretation:")
                print(f"  ‚Ä¢ For Œ¥ < {delta_star:.4f}: Clustered networks outperform")
                print(f"    ‚Üí Social reinforcement is more valuable")
                print(f"    ‚Üí Early, reinforced adoptions matter most")
                print(f"  ‚Ä¢ For Œ¥ > {delta_star:.4f}: Random networks outperform")
                print(f"    ‚Üí Network reach is more valuable")
                print(f"    ‚Üí Spreading widely over time matters most")
        else:
            # Œ¥* was not found in the tested range - provide detailed analysis
            print("\n" + "!"*70)
            print("*** Œ¥* NOT FOUND in the tested range ***")
            print("!"*70)

            # Analyze which network type dominates across the tested range
            differences = V_random_mean - V_clustered_mean
            random_wins = np.sum(differences > 0)
            clustered_wins = np.sum(differences < 0)
            total_tests = len(differences)

            # Calculate average magnitude of differences to assess closeness
            avg_abs_diff = np.mean(np.abs(differences))
            # Normalize by mean V values to get relative difference
            avg_V = (np.mean(V_clustered_mean) + np.mean(V_random_mean)) / 2
            relative_diff = avg_abs_diff / avg_V if avg_V > 0 else 0

            print(f"\nNetwork Performance Summary:")
            print(f"  ‚Ä¢ Random network wins: {random_wins}/{total_tests} delta values")
            print(f"  ‚Ä¢ Clustered network wins: {clustered_wins}/{total_tests} delta values")
            print(f"  ‚Ä¢ Average relative difference: {relative_diff:.2%}")

            # Determine dominant pattern and provide interpretation
            if random_wins == total_tests:
                # Random network always wins
                print(f"\n‚ö† Analysis: Random network ALWAYS outperforms in tested range")
                print(f"\nInterpretation:")
                print(f"  ‚Ä¢ Œ¥* is likely BELOW the tested range (Œ¥ < {delta_values[0]})")
                print(f"  ‚Ä¢ OR Œ¥* may not exist for these parameters")
                print(f"  ‚Ä¢ Network 'reach' dominates over 'reinforcement' across all tested Œ¥ values")
                print(f"  ‚Ä¢ Even when valuing early adoptions heavily, random networks spread better")
                print(f"\nüí° Recommendation:")
                print(f"  ‚Üí Test LOWER delta values: try [0.01, 0.02, 0.05, 0.1, ...]")
                print(f"  ‚Üí If Œ¥* exists, it's in the extreme early-adoption regime")

            elif clustered_wins == total_tests:
                # Clustered network always wins
                print(f"\n‚ö† Analysis: Clustered network ALWAYS outperforms in tested range")
                print(f"\nInterpretation:")
                print(f"  ‚Ä¢ Œ¥* is likely ABOVE the tested range (Œ¥ > {delta_values[-1]})")
                print(f"  ‚Ä¢ Social 'reinforcement' dominates over 'reach' across all tested Œ¥ values")
                print(f"  ‚Ä¢ Even when valuing late adoptions equally, clustered networks perform better")
                print(f"\nüí° Recommendation:")
                print(f"  ‚Üí Test HIGHER delta values: try [..., 0.995, 0.999, 0.9999]")
                print(f"  ‚Üí If Œ¥* exists, it's in the extreme late-adoption regime")

            elif relative_diff < 0.05:
                # Networks perform very similarly
                print(f"\n‚ö† Analysis: Networks perform VERY SIMILARLY across tested range")
                print(f"\nInterpretation:")
                print(f"  ‚Ä¢ Average difference is only {relative_diff:.2%} - negligible")
                print(f"  ‚Ä¢ Œ¥* may not be a meaningful concept for these parameters")
                print(f"  ‚Ä¢ Network structure has minimal impact on diffusion performance")
                print(f"\nüí° Recommendation:")
                print(f"  ‚Üí Consider testing different parameter combinations:")
                print(f"     - Different thresholds (current: i={thrshld})")
                print(f"     - Different p1/p2 ratios (current: p1={p1}, p2={p2})")
                print(f"     - Different beta values (current: Œ≤={beta})")

            else:
                # Mixed results - intersection might be near boundaries
                print(f"\n‚ö† Analysis: Mixed results - performance varies across range")
                print(f"\nInterpretation:")
                print(f"  ‚Ä¢ Both networks win in different regimes, but no clear intersection found")
                print(f"  ‚Ä¢ Œ¥* might be very close to one of the tested values")
                print(f"  ‚Ä¢ OR the transition is non-monotonic (curves cross multiple times)")
                print(f"\nüí° Recommendation:")
                print(f"  ‚Üí Test with FINER GRANULARITY around transition regions")
                print(f"  ‚Üí Current spacing: {delta_values[1]-delta_values[0]:.3f} to {delta_values[-1]-delta_values[-2]:.3f}")
                print(f"  ‚Üí Try denser grid: np.linspace({delta_values[0]}, {delta_values[-1]}, 50)")

            print("\n" + "!"*70)

        # ========== STEP 5: Visualize results ==========
        print("\n\nSTEP 5: Creating visualization")
        print("-" * 50)

        # Create and save the plot showing both curves and all Œ¥* intersections
        visualize_delta_analysis(
            delta_values, V_clustered_mean, V_clustered_se,
            V_random_mean, V_random_se, delta_star, all_intersections
            )

        # Determine dominant network structure
        dominant_info = determine_dominant_network(V_clustered_mean, V_random_mean, all_intersections)

        # Save detailed numerical results to CSV for further analysis
        results_summary = pd.DataFrame({
            'delta': delta_values,
            'V_clustered_mean': V_clustered_mean,
            'V_clustered_se': V_clustered_se,
            'V_random_mean': V_random_mean,
            'V_random_se': V_random_se,
            'difference': V_random_mean - V_clustered_mean,  # Positive = random better
            # New columns for dominant network analysis
            'delta_star_count': dominant_info['delta_star_count'],
            'delta_star_values': dominant_info['delta_star_values'],
            'dominant_below': dominant_info['dominant_below'],
            'dominant_above': dominant_info['dominant_above'],
            'always_dominant': dominant_info['always_dominant']
            })

        # Create output directory if it doesn't exist
        os.makedirs('../output_data', exist_ok=True)

        # Save to CSV
        output_file = '../output_data/delta_star_results.csv'
        results_summary.to_csv(output_file, index=False)
        print(f"\nDetailed results saved to: {output_file}")

        # Print completion message
        print("\n" + "="*70)
        print("ANALYSIS COMPLETE!")
        print("="*70)

        return delta_star, results_summary, all_intersections

    finally:
        # Restore stdout if it was redirected
        if old_stdout is not None:
            sys.stdout = old_stdout


# Execute the analysis when script is run directly (not imported as a module)
if __name__ == "__main__":
    # Run the complete analysis and store the results
    delta_star, results, all_ints = main()
